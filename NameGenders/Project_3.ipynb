{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1dbd84ed",
   "metadata": {},
   "source": [
    "## Project 3 - Names and Genders\n",
    "\n",
    "#### Summer 2021\n",
    "**Authors:** GOAT Team (Esteban Aramayo, Ethan Haley, Claire Meyer, and Tyler Frankenburg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60ebe7b",
   "metadata": {},
   "source": [
    "For Project 3, we'll try to predict genders of unseen names, based on genders of seen names.  \n",
    "\n",
    "We'll start with nltk's names corpus, complete some data preparations, then test different model types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "60931336",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "f8bedb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = nltk.corpus.names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0aea33",
   "metadata": {},
   "source": [
    "##### Exploration\n",
    "\n",
    "First, let's explore the names dataset a bit, and learn more about what's included. We can start by checking if the classes, Male and Female, will be balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "e6d9c833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2943 male names and 5001 female names.\n"
     ]
    }
   ],
   "source": [
    "print(f\"{len(names.words('male.txt'))} male names and {len(names.words('female.txt'))} female names.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6875364",
   "metadata": {},
   "source": [
    "The dataset overindexes on female names, at 63%. So, a majority classifier baseline accuracy would be 63%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "cbf7e986",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Ahmet', 'M')"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Put all names in one list, with gender labels\n",
    "names = [(n, 'M') for n in names.words('male.txt')] + \\\n",
    "        [(n, 'F') for n in names.words('female.txt')]\n",
    "\n",
    "# Shuffle names for ML, setting random seed so we get same results every time\n",
    "random.seed(620)\n",
    "random.shuffle(names)\n",
    "names[2345]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10629c34",
   "metadata": {},
   "source": [
    "What sorts of names are we dealing with here? Let's sample 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "83185bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Rahul', 'M'), ('Aida', 'F'), ('Emelda', 'F'), ('Cherey', 'F'), ('Tessi', 'F'), ('Marchelle', 'F'), ('Bobina', 'F'), ('Dewey', 'M'), ('Gavra', 'F'), ('Angie', 'F'), ('Emelina', 'F'), ('Jessalin', 'F'), ('Genevieve', 'F'), ('Giacinta', 'F'), ('Cloris', 'F'), ('Herman', 'M'), ('Vilhelm', 'M'), ('Kristine', 'F'), ('Elysha', 'F'), ('Marna', 'F')]\n"
     ]
    }
   ],
   "source": [
    "print(names[600:620])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b1054f",
   "metadata": {},
   "source": [
    "Looks a mix of many cultures, but weighted towards European languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "3db5ab96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Em', 'F')"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shortest name\n",
    "min(names, key=lambda n: len(n[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "47a88404",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Helen-Elizabeth', 'F')"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# longest name\n",
    "max(names, key=lambda n: len(n[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e573498c",
   "metadata": {},
   "source": [
    "We can replace those hyphens with spaces, make names lowercase, and add spaces at the start and end of each name too, to signify start and end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "8afe74ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(' helen elizabeth ', 'F')"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names = [(' ' + n[0].lower().replace('-', ' ') + ' ', n[1]) for n in names]\n",
    "max(names, key=lambda n: len(n[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51acd2ee",
   "metadata": {},
   "source": [
    "##### Data Preparation for Models\n",
    "\n",
    "Let's split names into train, dev, and test sets as specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c49cd87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tests, devs, trains = names[:500], names[500:1000], names[1000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7beda1b",
   "metadata": {},
   "source": [
    "The initial example from the book uses a Naive Bayes classifier, and ends with 76% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "0556c4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gender_features(word):\n",
    "    return {'suffix1': word[-1:],\n",
    "            'suffix2': word[-2:]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "a0dd3159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.76\n"
     ]
    }
   ],
   "source": [
    "train_set = [(gender_features(n), gender) for (n, gender) in trains]\n",
    "devtest_set = [(gender_features(n), gender) for (n, gender) in devs]\n",
    "test_set = [(gender_features(n), gender) for (n, gender) in tests]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print(nltk.classify.accuracy(classifier, devtest_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403127c5",
   "metadata": {},
   "source": [
    "As an example of feature engineering and extending the book's example, we could break every name down into its letter pairs.  So 27 * 27 possible pairs (with spaces)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "28286e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPairs(name):\n",
    "    '''\n",
    "    Given a lowercase name as input, this returns a dictionary \n",
    "    showing which pairs of letters are in the name.\n",
    "    Spaces are allowed, and only binary values are returned, not counts.\n",
    "    '''\n",
    "    letters = ' abcdefghijklmnopqrstuvwxyz'\n",
    "    pairdict = {p1+p2:False for p1 in letters for p2 in letters}\n",
    "    \n",
    "    for i in range(len(name)-1):\n",
    "        pairdict[name[i:i+2]] = True\n",
    "        \n",
    "    return pairdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "cbc1b794",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getPairs(' jo jo ')['o ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "6e30bc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use that function to make the features\n",
    "trainfeats = [(getPairs(n[0]), n[1]) for n in trains]\n",
    "devfeats = [(getPairs(n[0]), n[1]) for n in devs]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd10f64",
   "metadata": {},
   "source": [
    "##### Train and check accuracy of the models\n",
    "\n",
    "Then we create the classifiers themselves. We'll be creating both a Decision Tree and a Naive Bayes classifer and comparing the two, starting with **Naive Bayes**. We can implement the classifier on the training set, and check accuracy on our dev set, as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "1e752a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_model = nltk.NaiveBayesClassifier.train(trainfeats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "ecc901d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.816"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.classify.accuracy(nb_model, devfeats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1023a2",
   "metadata": {},
   "source": [
    "Just that one feature gets the model to 81.6% accuracy.  How much did it overfit on the training data? We can check by looking at the accuracy on the training set and comparing to dev."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "23c79416",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8224366359447005"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check model accuracy on training set\n",
    "nltk.classify.accuracy(nb_model, trainfeats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef577237",
   "metadata": {},
   "source": [
    "It doesn't look the the Na√Øve Bayes model overfit during training, so when we check it on the held out test names later, we should expect something similar to the dev results (81.6%)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e74da3a",
   "metadata": {},
   "source": [
    "Next let's try a **Decision Tree Classifier**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1a9dc222",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_model = nltk.DecisionTreeClassifier.train(trainfeats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9bf2ca9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check tree model accuracy on dev set\n",
    "nltk.classify.accuracy(tree_model, devfeats) ## 80.8%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bee632e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8649193548387096"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how had it done on the training?\n",
    "nltk.classify.accuracy(tree_model, trainfeats) ## 85.6%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13222728",
   "metadata": {},
   "source": [
    "The decision tree model overfit during training, and has slightly worse accuracy than the Bayes model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d592d92e",
   "metadata": {},
   "source": [
    "##### Learning from the Models\n",
    "\n",
    "Let's see what the models learned: of all our paired features, what were most informative?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1f464bd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                      a  = True                F : M      =     34.7 : 1.0\n",
      "                      f  = True                M : F      =     28.6 : 1.0\n",
      "                      k  = True                M : F      =     28.1 : 1.0\n",
      "                      rv = True                M : F      =     27.5 : 1.0\n",
      "                      fo = True                M : F      =     22.5 : 1.0\n",
      "                      lt = True                M : F      =     21.9 : 1.0\n",
      "                      hu = True                M : F      =     21.9 : 1.0\n",
      "                      iu = True                M : F      =     18.5 : 1.0\n",
      "                      rw = True                M : F      =     17.4 : 1.0\n",
      "                      v  = True                M : F      =     16.2 : 1.0\n",
      "                      p  = True                M : F      =     15.1 : 1.0\n",
      "                      sp = True                M : F      =     15.1 : 1.0\n",
      "                      rk = True                M : F      =     14.5 : 1.0\n",
      "                      dw = True                M : F      =     11.8 : 1.0\n",
      "                      wa = True                M : F      =     11.0 : 1.0\n",
      "                      um = True                M : F      =     10.6 : 1.0\n",
      "                      dl = True                M : F      =      9.5 : 1.0\n",
      "                      ez = True                M : F      =      9.5 : 1.0\n",
      "                      d  = True                M : F      =      9.5 : 1.0\n",
      "                      lf = True                M : F      =      8.8 : 1.0\n"
     ]
    }
   ],
   "source": [
    "nb_model.show_most_informative_features(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f257b454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "if d  == False: \n",
      "  if r  == False: \n",
      "    if s  == False: \n",
      "      if o  == False: \n",
      "        if n  == False: return 'M'\n",
      "        if n  == True: return 'F'\n",
      "      if o  == True: \n",
      "        if ko == False: return 'M'\n",
      "        if ko == True: return 'F'\n",
      "    if s  == True: \n",
      "      if is == False: \n",
      "        if ys == False: return 'F'\n",
      "        if ys == True: return 'F'\n",
      "      if is == True: \n",
      "        if ti == False: return 'M'\n",
      "        if ti == True: return 'M'\n",
      "  if r  == True: \n",
      "    if ni == False: \n",
      "      if no == False: \n",
      "        if mb == False: return 'M'\n",
      "        if mb == True: return 'F'\n",
      "      if no == True: \n",
      "        if ra == False: return 'F'\n",
      "        if ra == True: return 'M'\n",
      "    if ni == True: return 'F'\n",
      "if d  == True: \n",
      "  if sa == False: \n",
      "    if ag == False: \n",
      "      if dr == False: \n",
      "        if ng == False: return 'M'\n",
      "        if ng == True: return 'F'\n",
      "      if dr == True: return 'F'\n",
      "    if ag == True: return 'F'\n",
      "  if sa == True: \n",
      "    if ra == False: return 'F'\n",
      "    if ra == True: return 'M'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(tree_model.pseudocode(depth=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4dc4611",
   "metadata": {},
   "source": [
    "Based on most important features and the splits of the decision tree, the last letter is more important than the first letter.  Also notice that all these informative features other than the top one are indicators of male names.  This may be partly due to the fact that female names account for 63% of the names here. An attempt to squeeze a little more accuracy out of the model by balancing the training set, i.e. undersampling female names, led to a small dropoff in performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "5fd8cac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2943 male names and 5001 female names.\n"
     ]
    }
   ],
   "source": [
    "print(f\"{len(names.words('male.txt'))} male names and {len(names.words('female.txt'))} female names.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "6ba87c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_names = names.words('female.txt')\n",
    "random.seed(620)\n",
    "random.shuffle(f_names)\n",
    "f_dsample = f_names[:2942]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "51764709",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Wileen', 'F')"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Put all names in one list, with gender labels\n",
    "names_dsample = [(n, 'M') for n in names.words('male.txt')] + \\\n",
    "        [(n, 'F') for n in f_dsample]\n",
    "\n",
    "# Shuffle names for ML, setting random seed so we get same results every time\n",
    "random.seed(620)\n",
    "random.shuffle(names_dsample)\n",
    "names_dsample[2345]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "89de5e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "tests_d, devs_d, trains_d = names_dsample[:500], names_dsample[500:1000], names_dsample[1000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "b4f5af43",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainfeats_d = [(getPairs(n[0]), n[1]) for n in trains_d]\n",
    "devfeats_d = [(getPairs(n[0]), n[1]) for n in devs_d]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "463798c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_model_d = nltk.NaiveBayesClassifier.train(trainfeats_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "a0d2476f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.752"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.classify.accuracy(nb_model_d, devfeats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "b4a5bdd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7600806451612904"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.classify.accuracy(nb_model_d, trainfeats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07ebcfb",
   "metadata": {},
   "source": [
    "As we can see, accuracy actually dropped with balanced classes, to a result in line with the textbook's example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a88216",
   "metadata": {},
   "source": [
    "##### Implementing on our Test Sets\n",
    "\n",
    "Finally, we can look at the accuracy for our test set and see how it compares to the training and development sets. We expect comparable performance from our Naive Bayes model, and poorer performance from the Decision Tree classifier, which demonstrated overfitting.\n",
    "\n",
    "Even so, all three versions of this model are (though in the case of the downsampled model, very slight) improvements over the textbook's example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "2be43232",
   "metadata": {},
   "outputs": [],
   "source": [
    "testfeats = [(getPairs(n[0]), n[1]) for n in tests]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "6276d966",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Textbook example:  0.754\n",
      "Naive Bayes, all data:  0.822\n",
      "Decision Tree, all data:  0.796\n",
      "Naive Bayes, downsampled:  0.762\n"
     ]
    }
   ],
   "source": [
    "print('Textbook example: ', nltk.classify.accuracy(classifier, test_set))\n",
    "print('Naive Bayes, all data: ', nltk.classify.accuracy(nb_model, testfeats))\n",
    "print('Decision Tree, all data: ', nltk.classify.accuracy(tree_model, testfeats))\n",
    "print('Naive Bayes, downsampled: ', nltk.classify.accuracy(nb_model_d, testfeats))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
