{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9c46774",
   "metadata": {},
   "source": [
    "## Assignment 5-6: Document Classification\n",
    "#### Summer 2021\n",
    "**Authors:** GOAT Team (Esteban Aramayo, Ethan Haley, Claire Meyer, and Tyler Frankenburg)\n",
    "\n",
    "In this assignment, we'll ingest a dataset on emails from the [UCI Machine Learning Repository](http://archive.ics.uci.edu/ml/datasets/Spambase) and build a classifier to predict if a row is a spam email or not, using the provided features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3edd6fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5f24f4",
   "metadata": {},
   "source": [
    "##### Configuring the Spam dataset\n",
    "\n",
    "First we'll import the email data into a CSV without headers, as we'll add the column names from the names file later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ff56091",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.756</td>\n",
       "      <td>61</td>\n",
       "      <td>278</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.21</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.94</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.048</td>\n",
       "      <td>5.114</td>\n",
       "      <td>101</td>\n",
       "      <td>1028</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.010</td>\n",
       "      <td>9.821</td>\n",
       "      <td>485</td>\n",
       "      <td>2259</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     0     1     2    3     4     5     6     7     8     9   ...    48  \\\n",
       "0  0.00  0.64  0.64  0.0  0.32  0.00  0.00  0.00  0.00  0.00  ...  0.00   \n",
       "1  0.21  0.28  0.50  0.0  0.14  0.28  0.21  0.07  0.00  0.94  ...  0.00   \n",
       "2  0.06  0.00  0.71  0.0  1.23  0.19  0.19  0.12  0.64  0.25  ...  0.01   \n",
       "3  0.00  0.00  0.00  0.0  0.63  0.00  0.31  0.63  0.31  0.63  ...  0.00   \n",
       "4  0.00  0.00  0.00  0.0  0.63  0.00  0.31  0.63  0.31  0.63  ...  0.00   \n",
       "\n",
       "      49   50     51     52     53     54   55    56  57  \n",
       "0  0.000  0.0  0.778  0.000  0.000  3.756   61   278   1  \n",
       "1  0.132  0.0  0.372  0.180  0.048  5.114  101  1028   1  \n",
       "2  0.143  0.0  0.276  0.184  0.010  9.821  485  2259   1  \n",
       "3  0.137  0.0  0.137  0.000  0.000  3.537   40   191   1  \n",
       "4  0.135  0.0  0.135  0.000  0.000  3.537   40   191   1  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_data = pd.read_csv(\"https://raw.githubusercontent.com/ebhtra/gory-graph/main/DocumentClassification/spambase.data\", header=None)\n",
    "\n",
    "spam_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734367f0",
   "metadata": {},
   "source": [
    "Then we open the names file, which includes names for all our eventual features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "726c874f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "\n",
    "link = \"https://raw.githubusercontent.com/ebhtra/gory-graph/main/DocumentClassification/spambase.names\"\n",
    "f = urllib.request.urlopen(link)\n",
    "f = f.read().decode()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a542a7b0",
   "metadata": {},
   "source": [
    "Using Regex findall(), we can use pattern matching to ignore documentation text and pull in all the feature names to use as columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c6e342c",
   "metadata": {},
   "outputs": [],
   "source": [
    "colnames = re.findall(\"word_freq_[a-z]*:|word_freq_[a-z]*[0-9]*[a-z]:|word_freq_[a-z]*[0-9]*:|char_freq_.:|capital_run_length_[a-z]*:\",f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a87cf723",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(colnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "186150ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "colnames.append(\"spam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d3e62cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58\n"
     ]
    }
   ],
   "source": [
    "print(len(colnames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d62698a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_data.columns = colnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8dda86d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_freq_make:</th>\n",
       "      <th>word_freq_address:</th>\n",
       "      <th>word_freq_all:</th>\n",
       "      <th>word_freq_3d:</th>\n",
       "      <th>word_freq_our:</th>\n",
       "      <th>word_freq_over:</th>\n",
       "      <th>word_freq_remove:</th>\n",
       "      <th>word_freq_internet:</th>\n",
       "      <th>word_freq_order:</th>\n",
       "      <th>word_freq_mail:</th>\n",
       "      <th>...</th>\n",
       "      <th>char_freq_;:</th>\n",
       "      <th>char_freq_(:</th>\n",
       "      <th>char_freq_[:</th>\n",
       "      <th>char_freq_!:</th>\n",
       "      <th>char_freq_$:</th>\n",
       "      <th>char_freq_#:</th>\n",
       "      <th>capital_run_length_average:</th>\n",
       "      <th>capital_run_length_longest:</th>\n",
       "      <th>capital_run_length_total:</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.756</td>\n",
       "      <td>61</td>\n",
       "      <td>278</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.21</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.94</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.048</td>\n",
       "      <td>5.114</td>\n",
       "      <td>101</td>\n",
       "      <td>1028</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.010</td>\n",
       "      <td>9.821</td>\n",
       "      <td>485</td>\n",
       "      <td>2259</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   word_freq_make:  word_freq_address:  word_freq_all:  word_freq_3d:  \\\n",
       "0             0.00                0.64            0.64            0.0   \n",
       "1             0.21                0.28            0.50            0.0   \n",
       "2             0.06                0.00            0.71            0.0   \n",
       "3             0.00                0.00            0.00            0.0   \n",
       "4             0.00                0.00            0.00            0.0   \n",
       "\n",
       "   word_freq_our:  word_freq_over:  word_freq_remove:  word_freq_internet:  \\\n",
       "0            0.32             0.00               0.00                 0.00   \n",
       "1            0.14             0.28               0.21                 0.07   \n",
       "2            1.23             0.19               0.19                 0.12   \n",
       "3            0.63             0.00               0.31                 0.63   \n",
       "4            0.63             0.00               0.31                 0.63   \n",
       "\n",
       "   word_freq_order:  word_freq_mail:  ...  char_freq_;:  char_freq_(:  \\\n",
       "0              0.00             0.00  ...          0.00         0.000   \n",
       "1              0.00             0.94  ...          0.00         0.132   \n",
       "2              0.64             0.25  ...          0.01         0.143   \n",
       "3              0.31             0.63  ...          0.00         0.137   \n",
       "4              0.31             0.63  ...          0.00         0.135   \n",
       "\n",
       "   char_freq_[:  char_freq_!:  char_freq_$:  char_freq_#:  \\\n",
       "0           0.0         0.778         0.000         0.000   \n",
       "1           0.0         0.372         0.180         0.048   \n",
       "2           0.0         0.276         0.184         0.010   \n",
       "3           0.0         0.137         0.000         0.000   \n",
       "4           0.0         0.135         0.000         0.000   \n",
       "\n",
       "   capital_run_length_average:  capital_run_length_longest:  \\\n",
       "0                        3.756                           61   \n",
       "1                        5.114                          101   \n",
       "2                        9.821                          485   \n",
       "3                        3.537                           40   \n",
       "4                        3.537                           40   \n",
       "\n",
       "   capital_run_length_total:  spam  \n",
       "0                        278     1  \n",
       "1                       1028     1  \n",
       "2                       2259     1  \n",
       "3                        191     1  \n",
       "4                        191     1  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff529d0",
   "metadata": {},
   "source": [
    "##### Exploring the Data\n",
    "\n",
    "Let's look at class balance, as well as descriptive statistics of each field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1f0f6d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_freq_make:</th>\n",
       "      <th>word_freq_address:</th>\n",
       "      <th>word_freq_all:</th>\n",
       "      <th>word_freq_3d:</th>\n",
       "      <th>word_freq_our:</th>\n",
       "      <th>word_freq_over:</th>\n",
       "      <th>word_freq_remove:</th>\n",
       "      <th>word_freq_internet:</th>\n",
       "      <th>word_freq_order:</th>\n",
       "      <th>word_freq_mail:</th>\n",
       "      <th>...</th>\n",
       "      <th>char_freq_;:</th>\n",
       "      <th>char_freq_(:</th>\n",
       "      <th>char_freq_[:</th>\n",
       "      <th>char_freq_!:</th>\n",
       "      <th>char_freq_$:</th>\n",
       "      <th>char_freq_#:</th>\n",
       "      <th>capital_run_length_average:</th>\n",
       "      <th>capital_run_length_longest:</th>\n",
       "      <th>capital_run_length_total:</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.104553</td>\n",
       "      <td>0.213015</td>\n",
       "      <td>0.280656</td>\n",
       "      <td>0.065425</td>\n",
       "      <td>0.312223</td>\n",
       "      <td>0.095901</td>\n",
       "      <td>0.114208</td>\n",
       "      <td>0.105295</td>\n",
       "      <td>0.090067</td>\n",
       "      <td>0.239413</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038575</td>\n",
       "      <td>0.139030</td>\n",
       "      <td>0.016976</td>\n",
       "      <td>0.269071</td>\n",
       "      <td>0.075811</td>\n",
       "      <td>0.044238</td>\n",
       "      <td>5.191515</td>\n",
       "      <td>52.172789</td>\n",
       "      <td>283.289285</td>\n",
       "      <td>0.394045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.305358</td>\n",
       "      <td>1.290575</td>\n",
       "      <td>0.504143</td>\n",
       "      <td>1.395151</td>\n",
       "      <td>0.672513</td>\n",
       "      <td>0.273824</td>\n",
       "      <td>0.391441</td>\n",
       "      <td>0.401071</td>\n",
       "      <td>0.278616</td>\n",
       "      <td>0.644755</td>\n",
       "      <td>...</td>\n",
       "      <td>0.243471</td>\n",
       "      <td>0.270355</td>\n",
       "      <td>0.109394</td>\n",
       "      <td>0.815672</td>\n",
       "      <td>0.245882</td>\n",
       "      <td>0.429342</td>\n",
       "      <td>31.729449</td>\n",
       "      <td>194.891310</td>\n",
       "      <td>606.347851</td>\n",
       "      <td>0.488698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.588000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.065000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.276000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>95.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.380000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.188000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.315000</td>\n",
       "      <td>0.052000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.706000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>266.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4.540000</td>\n",
       "      <td>14.280000</td>\n",
       "      <td>5.100000</td>\n",
       "      <td>42.810000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>5.880000</td>\n",
       "      <td>7.270000</td>\n",
       "      <td>11.110000</td>\n",
       "      <td>5.260000</td>\n",
       "      <td>18.180000</td>\n",
       "      <td>...</td>\n",
       "      <td>4.385000</td>\n",
       "      <td>9.752000</td>\n",
       "      <td>4.081000</td>\n",
       "      <td>32.478000</td>\n",
       "      <td>6.003000</td>\n",
       "      <td>19.829000</td>\n",
       "      <td>1102.500000</td>\n",
       "      <td>9989.000000</td>\n",
       "      <td>15841.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       word_freq_make:  word_freq_address:  word_freq_all:  word_freq_3d:  \\\n",
       "count      4601.000000         4601.000000     4601.000000    4601.000000   \n",
       "mean          0.104553            0.213015        0.280656       0.065425   \n",
       "std           0.305358            1.290575        0.504143       1.395151   \n",
       "min           0.000000            0.000000        0.000000       0.000000   \n",
       "25%           0.000000            0.000000        0.000000       0.000000   \n",
       "50%           0.000000            0.000000        0.000000       0.000000   \n",
       "75%           0.000000            0.000000        0.420000       0.000000   \n",
       "max           4.540000           14.280000        5.100000      42.810000   \n",
       "\n",
       "       word_freq_our:  word_freq_over:  word_freq_remove:  \\\n",
       "count     4601.000000      4601.000000        4601.000000   \n",
       "mean         0.312223         0.095901           0.114208   \n",
       "std          0.672513         0.273824           0.391441   \n",
       "min          0.000000         0.000000           0.000000   \n",
       "25%          0.000000         0.000000           0.000000   \n",
       "50%          0.000000         0.000000           0.000000   \n",
       "75%          0.380000         0.000000           0.000000   \n",
       "max         10.000000         5.880000           7.270000   \n",
       "\n",
       "       word_freq_internet:  word_freq_order:  word_freq_mail:  ...  \\\n",
       "count          4601.000000       4601.000000      4601.000000  ...   \n",
       "mean              0.105295          0.090067         0.239413  ...   \n",
       "std               0.401071          0.278616         0.644755  ...   \n",
       "min               0.000000          0.000000         0.000000  ...   \n",
       "25%               0.000000          0.000000         0.000000  ...   \n",
       "50%               0.000000          0.000000         0.000000  ...   \n",
       "75%               0.000000          0.000000         0.160000  ...   \n",
       "max              11.110000          5.260000        18.180000  ...   \n",
       "\n",
       "       char_freq_;:  char_freq_(:  char_freq_[:  char_freq_!:  char_freq_$:  \\\n",
       "count   4601.000000   4601.000000   4601.000000   4601.000000   4601.000000   \n",
       "mean       0.038575      0.139030      0.016976      0.269071      0.075811   \n",
       "std        0.243471      0.270355      0.109394      0.815672      0.245882   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%        0.000000      0.065000      0.000000      0.000000      0.000000   \n",
       "75%        0.000000      0.188000      0.000000      0.315000      0.052000   \n",
       "max        4.385000      9.752000      4.081000     32.478000      6.003000   \n",
       "\n",
       "       char_freq_#:  capital_run_length_average:  capital_run_length_longest:  \\\n",
       "count   4601.000000                  4601.000000                  4601.000000   \n",
       "mean       0.044238                     5.191515                    52.172789   \n",
       "std        0.429342                    31.729449                   194.891310   \n",
       "min        0.000000                     1.000000                     1.000000   \n",
       "25%        0.000000                     1.588000                     6.000000   \n",
       "50%        0.000000                     2.276000                    15.000000   \n",
       "75%        0.000000                     3.706000                    43.000000   \n",
       "max       19.829000                  1102.500000                  9989.000000   \n",
       "\n",
       "       capital_run_length_total:         spam  \n",
       "count                4601.000000  4601.000000  \n",
       "mean                  283.289285     0.394045  \n",
       "std                   606.347851     0.488698  \n",
       "min                     1.000000     0.000000  \n",
       "25%                    35.000000     0.000000  \n",
       "50%                    95.000000     0.000000  \n",
       "75%                   266.000000     1.000000  \n",
       "max                 15841.000000     1.000000  \n",
       "\n",
       "[8 rows x 58 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c6f5bf16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2788\n",
       "1    1813\n",
       "Name: spam, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_data['spam'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154caa6f",
   "metadata": {},
   "source": [
    "This dataset contains just under 40% spam, so the classes are not perfectly balanced.  \n",
    "\n",
    "The capital letter counts are on a different scale from the string frequencies, so we'll need to normalize features for certain ML models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbe19af",
   "metadata": {},
   "source": [
    "##### Building the classifiers\n",
    "\n",
    "We're going to squeeze what we can out of a few sci-kit learn classification models. To start, we'll split our target, y, from our features, X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "083b17ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = spam_data.iloc[:,57]\n",
    "X = spam_data.iloc[:,:57]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46718cd2",
   "metadata": {},
   "source": [
    "Here are the features for the first email:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "59de75c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_freq_make:                  0.000\n",
      "word_freq_address:               0.640\n",
      "word_freq_all:                   0.640\n",
      "word_freq_3d:                    0.000\n",
      "word_freq_our:                   0.320\n",
      "word_freq_over:                  0.000\n",
      "word_freq_remove:                0.000\n",
      "word_freq_internet:              0.000\n",
      "word_freq_order:                 0.000\n",
      "word_freq_mail:                  0.000\n",
      "word_freq_receive:               0.000\n",
      "word_freq_will:                  0.640\n",
      "word_freq_people:                0.000\n",
      "word_freq_report:                0.000\n",
      "word_freq_addresses:             0.000\n",
      "word_freq_free:                  0.320\n",
      "word_freq_business:              0.000\n",
      "word_freq_email:                 1.290\n",
      "word_freq_you:                   1.930\n",
      "word_freq_credit:                0.000\n",
      "word_freq_your:                  0.960\n",
      "word_freq_font:                  0.000\n",
      "word_freq_000:                   0.000\n",
      "word_freq_money:                 0.000\n",
      "word_freq_hp:                    0.000\n",
      "word_freq_hpl:                   0.000\n",
      "word_freq_george:                0.000\n",
      "word_freq_650:                   0.000\n",
      "word_freq_lab:                   0.000\n",
      "word_freq_labs:                  0.000\n",
      "word_freq_telnet:                0.000\n",
      "word_freq_857:                   0.000\n",
      "word_freq_data:                  0.000\n",
      "word_freq_415:                   0.000\n",
      "word_freq_85:                    0.000\n",
      "word_freq_technology:            0.000\n",
      "word_freq_1999:                  0.000\n",
      "word_freq_parts:                 0.000\n",
      "word_freq_pm:                    0.000\n",
      "word_freq_direct:                0.000\n",
      "word_freq_cs:                    0.000\n",
      "word_freq_meeting:               0.000\n",
      "word_freq_original:              0.000\n",
      "word_freq_project:               0.000\n",
      "word_freq_re:                    0.000\n",
      "word_freq_edu:                   0.000\n",
      "word_freq_table:                 0.000\n",
      "word_freq_conference:            0.000\n",
      "char_freq_;:                     0.000\n",
      "char_freq_(:                     0.000\n",
      "char_freq_[:                     0.000\n",
      "char_freq_!:                     0.778\n",
      "char_freq_$:                     0.000\n",
      "char_freq_#:                     0.000\n",
      "capital_run_length_average:      3.756\n",
      "capital_run_length_longest:     61.000\n",
      "capital_run_length_total:      278.000\n",
      "Name: 0, dtype: float64\n",
      "\n",
      "email is spam: 1\n"
     ]
    }
   ],
   "source": [
    "print(X.iloc[0,:])\n",
    "print()\n",
    "print(f\"email is spam: {y[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4910288a",
   "metadata": {},
   "source": [
    "There are frequency counts (how many times does this string appear in the email, for every 100 strings in the email) for 48 chosen strings, as well as 6 chosen characters.  There are also 3 different measures of capital letters.  Aside from trying to create our own non-linear combinations of the provided features, our feature engineering capabilities are handcuffed in this project, since we don't have the emails from which these features were chosen.  As such, we'll focus our efforts on leveraging sklearn's ML tools and best practices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df092ddc",
   "metadata": {},
   "source": [
    "We'll split into test and train sets using sklearn's built-in splitting function. We'll set aside 10% of the samples as a holdout for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "83c37729",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(X, y, test_size=.1, random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1a790b91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resulting lengths of \n",
      "X_train, y_train, X_test, and y_test:\n",
      "[4140, 4140, 461, 461]\n"
     ]
    }
   ],
   "source": [
    "print('Resulting lengths of ')\n",
    "print('X_train, y_train, X_test, and y_test:')\n",
    "print(list(map(len, (X_train, y_train, X_test, y_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677825ac",
   "metadata": {},
   "source": [
    "To allow classifiers like logistic regression to learn best, we'll normalize and shrink our features using sklearn's StandardScaler, which maps an array of numbers to a unit normal distribution by subtracting the mean of the array and dividing by its std.dev."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "abfa7dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)  \n",
    "X_scale_train = scaler.transform(X_train)\n",
    "X_scale_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d30c0bb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features now range from -0.9357153821855062 to 49.33457588472551\n"
     ]
    }
   ],
   "source": [
    "print(f'Features now range from {np.min(X_scale_train)} to {np.max(X_scale_train)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04f55db",
   "metadata": {},
   "source": [
    "That maximum value 49 std.devs above the mean isn't ideal, but somehow one of the emails has over 15000 capitals. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aeb14b3",
   "metadata": {},
   "source": [
    "But in order to use k-fold cross-validation during the training of a model, it's better to fit the scaler to each training split and then transform the held-out fold with that current iteration of the scaler, thus avoiding data leakage within the process.  Rather than implementing it from scratch, we'll use a sklearn Pipeline object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "354b1321",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([('scaler', StandardScaler()), ('model', LogisticRegression(random_state=1234, max_iter=500))])\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1234)\n",
    "scores = cross_validate(pipe, X_train, y_train, scoring=('accuracy', 'precision'),\n",
    "                        cv=cv, return_train_score=True, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b91e548",
   "metadata": {},
   "source": [
    "We began with Logistic Regression here because it's well suited to the type of features that were given, and allows us to get a good idea of which of those features are generally important or not.  We chose accuracy and precision as scoring metrics because of the nature of the classification task, spam filtering.  Besides just finding the most accurate filter in terms of correctly classifying emails, we want a filter with a high precision score, assuming we'd rather see a few spam emails in our inbox than to find a few important emails in the spam folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a244395c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean accuracy during training 10 splits 3 different times was 0.9288\n",
      "The standard deviation of those 30 scores was 0.002\n",
      "\n",
      "The mean scores for the 30 held out validation folds was 0.9241\n",
      "Their st.dev. was 0.0085\n"
     ]
    }
   ],
   "source": [
    "print(f\"The mean accuracy during training 10 splits 3 different times was {round(np.mean(scores['train_accuracy']),4)}\")\n",
    "print(f\"The standard deviation of those 30 scores was {round(np.std(scores['train_accuracy']),4)}\")\n",
    "print()\n",
    "print(f\"The mean scores for the 30 held out validation folds was {round(np.mean(scores['test_accuracy']),4)}\")\n",
    "print(f\"Their st.dev. was {round(np.std(scores['test_accuracy']),4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0edb36f",
   "metadata": {},
   "source": [
    "**The validation accuracies are just a half percent worse than the training ones, so there's not much evidence of overfitting.  With the 90-10 split provided by the 10 folds, we'd like to see the smaller (validation) folds showing $\\sqrt{9}$ = 3 times higher stdev. than the training splits, which are 9 times larger.  Here we see that the ratio is actually 4.25 (8.5e-3 vs. 2e-3), so the 92.41% validation accuracy is a little unstable.** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45bec2e6",
   "metadata": {},
   "source": [
    "Let's see what the precisions were."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7e81ea9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean test precision was 0.9223\n"
     ]
    }
   ],
   "source": [
    "print(f\"The mean test precision was {round(np.mean(scores['test_precision']),4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9787c993",
   "metadata": {},
   "source": [
    "Since we used `RepeatedStratifiedKFold` for cross-validation, there should be about 40% spam in each fold, and since the model was about 93% accurate, it was probably predicting `spam` about 40% of the time.  With the precision score around 92%, this means we could expect around 8% False Positives on 40% spam predictions, or around 3% False Positives overall, where a non-spam email got sent to the spam box by mistake.  This may or may not be considered acceptable to a user of the filter, but we'll hope to do better with other models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d613d8",
   "metadata": {},
   "source": [
    "We might at this point choose to try out different regularization penalties (lasso and ridge regression) or to prune some of the 57 features we were given, but since there's little evidence of overfitting, we'll just retrain a logistic regression model on the entire training set and verify that it gets something like 92.5% accuracy with 3% false positives on the unseen 10% of the data we split off for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "96b85ac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy is 0.9306\n",
      "False positive rate is 0.05\n"
     ]
    }
   ],
   "source": [
    "# Use the scaled data from before\n",
    "linear_reg = LogisticRegression(max_iter=300, random_state=620).fit(X_scale_train, y_train)\n",
    "lin_pred = linear_reg.predict(X_scale_test)\n",
    "lin_score = accuracy_score(y_test, lin_pred)\n",
    "print(f\"Test accuracy is {round(lin_score,4)}\")\n",
    "print(f\"False positive rate is {round(sum(lin_pred - y_test == 1) / sum(y_test==0), 3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c6afa059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test precision is 0.9218\n"
     ]
    }
   ],
   "source": [
    "print(f\"Test precision is {round(np.dot(y_test, lin_pred) / sum(lin_pred),4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995b8750",
   "metadata": {},
   "source": [
    "The test precision, accuracy, and therefore false positives, are as expected.  But it didn't necessarily have to turn out that way, with random variance in how any single data split happens to work, or how any one classifier happens to learn.  Let's just see how the 30 cross-validation models ranged:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "28001a2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Worst cross-val accuracy out of 30 splits was 0.9082\n",
      "Best was 0.942\n"
     ]
    }
   ],
   "source": [
    "print(f\"Worst cross-val accuracy out of 30 splits was {round(min(scores['test_accuracy']),4)}\")\n",
    "print(f\"Best was {round(max(scores['test_accuracy']),4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818b3bcc",
   "metadata": {},
   "source": [
    "If we just chose which classifier to use for our spam filter based on how one test batch happened to perform, we might easily end up with the wrong choice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f9e198",
   "metadata": {},
   "source": [
    "**What were the 20 most important features?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2069b5b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(-4.342655119799001, 'word_freq_george:'),\n",
       " (-2.6306171946409926, 'word_freq_hp:'),\n",
       " (-1.9651440530264461, 'word_freq_cs:'),\n",
       " (-1.483445991016186, 'word_freq_meeting:'),\n",
       " (1.3629780202839097, 'char_freq_$:'),\n",
       " (1.1582746904568435, 'capital_run_length_longest:'),\n",
       " (-1.1264679265650326, 'word_freq_lab:'),\n",
       " (-1.1225967091537659, 'word_freq_edu:'),\n",
       " (1.0141596061160116, 'word_freq_3d:'),\n",
       " (-0.9822463528948014, 'word_freq_hpl:'),\n",
       " (-0.9645475189839162, 'word_freq_85:'),\n",
       " (-0.9253869844251749, 'word_freq_project:'),\n",
       " (0.8852594401809076, 'word_freq_000:'),\n",
       " (0.862935252144216, 'word_freq_free:'),\n",
       " (0.8626591907835535, 'word_freq_remove:'),\n",
       " (-0.8439616518801258, 'word_freq_re:'),\n",
       " (-0.8257336316697627, 'word_freq_conference:'),\n",
       " (0.8116290737236534, 'char_freq_#:'),\n",
       " (0.5352714603142892, 'word_freq_credit:'),\n",
       " (-0.5339219619128391, 'word_freq_telnet:')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(list(zip(linear_reg.coef_[0], X.columns)), key=lambda x: abs(x[0]), reverse=True)[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741684cb",
   "metadata": {},
   "source": [
    "**\"$\", long runs of capitals, \"3d\", \"000\", \"free\", and \"remove\" all suggest spam.**  \n",
    "\n",
    "**\"george\", \"hp\", \"cs\", \"meeting\", \"lab\", and \"edu\" suggest non-spam.** \n",
    "\n",
    "Let's suppose these emails come from George who works at HP. It's debatable whether the frequency of the strings \"George\" and \"HP\" should be considered indications of non-spam.  If you want to train a separate classifier for every person then it makes sense to use their names.  But if you want to train a general purpose model, you wouldn't use names.  Still, this training set gives us no room to engineer other features, since we are just given a list of 57 of them, so perhaps it makes sense to use the name features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a0d94f",
   "metadata": {},
   "source": [
    "Another topic of interest is how changing the model's threshold for classification changes the accuracy and precision.\n",
    "Can we raise accuracy without letting too many important emails go to the spam folder (by lowering the threshold)?\n",
    "Can we reduce false positives without hurting accuracy too much (by raising the threshold)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b46ee00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def threshold(thresh, probs, truths):\n",
    "    '''Use your own threshold as first argument, to see how\n",
    "    accuracy and false positive rates change for given (2-D) probability\n",
    "    array and ground truths as 2nd and 3rd arguments.\n",
    "    '''\n",
    "    assert len(truths) == probs.shape[0], print('X and y have different lengths')\n",
    "    preds = [p[1] > thresh for p in probs]\n",
    "    print(f\"Threshold set to {thresh}\")\n",
    "    print(f\"Test accuracy is {round(accuracy_score(truths, preds), 4)}\")\n",
    "    print(f\"False positive rate is {round(sum(preds - truths == 1) / sum(truths==0), 3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d91bd46",
   "metadata": {},
   "source": [
    "Now we'll ask the same model not for binary predictions of spam, but for probabilities of being spam, and then we can feed those probabilities into the threshold function we just made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "affbb7c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold set to 0.5\n",
      "Test accuracy is 0.9306\n",
      "False positive rate is 0.05\n"
     ]
    }
   ],
   "source": [
    "# default used by sklearn predict functions is 0.5, so this should match above scores\n",
    "lin_prob = linear_reg.predict_proba(X_scale_test)\n",
    "truths = y_test\n",
    "threshold(0.5, lin_prob, truths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc7ac86",
   "metadata": {},
   "source": [
    "Raise threshold from sklearn's default 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2c47f808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold set to 0.67\n",
      "Test accuracy is 0.9219\n",
      "False positive rate is 0.025\n"
     ]
    }
   ],
   "source": [
    "threshold(0.67, lin_prob, truths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af195022",
   "metadata": {},
   "source": [
    "**To reduce false positives by a half, we only had to lower accuracy from 0.931 to 0.922**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a084ae1a",
   "metadata": {},
   "source": [
    "Lower threshold from 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "43900188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold set to 0.4\n",
      "Test accuracy is 0.9328\n",
      "False positive rate is 0.076\n"
     ]
    }
   ],
   "source": [
    "threshold(0.40, lin_prob, truths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9baeca",
   "metadata": {},
   "source": [
    "**This direction looks worse:  Our FP rate went up 50% and we barely improved accuracy**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d94ba70",
   "metadata": {},
   "source": [
    "##### Decision Trees\n",
    "\n",
    "Now we'll try some decision trees to see how they compare.  We won't use a pipeline for the cross-validation scaling here, since decision trees split on the order of the feature values, and the order won't be changed by scaling.  We will, however, use sklearn's `GridSearchCV` to find the best hyperparameters via cross-validation.  For decision trees, these hyperparameters include the depth of the tree and the minimum number of nodes needed in a leaf in order to split on it (amongst others). Both of those parameters are used to control the tradeoff between underfitting and overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "78dda773",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, estimator=DecisionTreeClassifier(random_state=1234),\n",
       "             n_jobs=-1,\n",
       "             param_grid=[{'max_depth': [3, 6, 9, 12],\n",
       "                          'min_samples_split': [2, 3, 4, 5]}],\n",
       "             return_train_score=True, scoring='accuracy')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_params_for_trees = [{\n",
    "    'max_depth': [3,6,9,12],\n",
    "    'min_samples_split': [2,3,4,5]}]\n",
    "\n",
    "CV_trees = GridSearchCV(estimator = DecisionTreeClassifier(random_state=1234), scoring='accuracy',\n",
    "                               param_grid = grid_params_for_trees,\n",
    "                               cv = 10, return_train_score=True, n_jobs=-1)\n",
    "\n",
    "CV_trees.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a1c2410a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_min_samples_split</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>...</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>split3_train_score</th>\n",
       "      <th>split4_train_score</th>\n",
       "      <th>split5_train_score</th>\n",
       "      <th>split6_train_score</th>\n",
       "      <th>split7_train_score</th>\n",
       "      <th>split8_train_score</th>\n",
       "      <th>split9_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.063571</td>\n",
       "      <td>0.003762</td>\n",
       "      <td>0.003685</td>\n",
       "      <td>0.000208</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>{'max_depth': 12, 'min_samples_split': 4}</td>\n",
       "      <td>0.908213</td>\n",
       "      <td>0.915459</td>\n",
       "      <td>0.946860</td>\n",
       "      <td>...</td>\n",
       "      <td>0.977187</td>\n",
       "      <td>0.972356</td>\n",
       "      <td>0.974772</td>\n",
       "      <td>0.972088</td>\n",
       "      <td>0.975040</td>\n",
       "      <td>0.968867</td>\n",
       "      <td>0.973162</td>\n",
       "      <td>0.971820</td>\n",
       "      <td>0.973349</td>\n",
       "      <td>0.002239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.050704</td>\n",
       "      <td>0.001575</td>\n",
       "      <td>0.003468</td>\n",
       "      <td>0.000191</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>{'max_depth': 9, 'min_samples_split': 4}</td>\n",
       "      <td>0.905797</td>\n",
       "      <td>0.910628</td>\n",
       "      <td>0.934783</td>\n",
       "      <td>...</td>\n",
       "      <td>0.961621</td>\n",
       "      <td>0.957327</td>\n",
       "      <td>0.962158</td>\n",
       "      <td>0.958132</td>\n",
       "      <td>0.957059</td>\n",
       "      <td>0.954643</td>\n",
       "      <td>0.961084</td>\n",
       "      <td>0.958937</td>\n",
       "      <td>0.959071</td>\n",
       "      <td>0.002310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.058885</td>\n",
       "      <td>0.005512</td>\n",
       "      <td>0.003046</td>\n",
       "      <td>0.000711</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>{'max_depth': 12, 'min_samples_split': 5}</td>\n",
       "      <td>0.903382</td>\n",
       "      <td>0.903382</td>\n",
       "      <td>0.932367</td>\n",
       "      <td>...</td>\n",
       "      <td>0.975309</td>\n",
       "      <td>0.971014</td>\n",
       "      <td>0.973430</td>\n",
       "      <td>0.970209</td>\n",
       "      <td>0.974235</td>\n",
       "      <td>0.967525</td>\n",
       "      <td>0.971820</td>\n",
       "      <td>0.969404</td>\n",
       "      <td>0.971900</td>\n",
       "      <td>0.002389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.061927</td>\n",
       "      <td>0.002012</td>\n",
       "      <td>0.003354</td>\n",
       "      <td>0.000123</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>{'max_depth': 12, 'min_samples_split': 2}</td>\n",
       "      <td>0.903382</td>\n",
       "      <td>0.908213</td>\n",
       "      <td>0.946860</td>\n",
       "      <td>...</td>\n",
       "      <td>0.977456</td>\n",
       "      <td>0.974503</td>\n",
       "      <td>0.976382</td>\n",
       "      <td>0.973430</td>\n",
       "      <td>0.976919</td>\n",
       "      <td>0.970478</td>\n",
       "      <td>0.973698</td>\n",
       "      <td>0.974235</td>\n",
       "      <td>0.974960</td>\n",
       "      <td>0.002197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.061335</td>\n",
       "      <td>0.002572</td>\n",
       "      <td>0.003652</td>\n",
       "      <td>0.000465</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>{'max_depth': 12, 'min_samples_split': 3}</td>\n",
       "      <td>0.896135</td>\n",
       "      <td>0.908213</td>\n",
       "      <td>0.942029</td>\n",
       "      <td>...</td>\n",
       "      <td>0.977456</td>\n",
       "      <td>0.973698</td>\n",
       "      <td>0.975309</td>\n",
       "      <td>0.972893</td>\n",
       "      <td>0.976114</td>\n",
       "      <td>0.970478</td>\n",
       "      <td>0.973698</td>\n",
       "      <td>0.973430</td>\n",
       "      <td>0.974450</td>\n",
       "      <td>0.002156</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "14       0.063571      0.003762         0.003685        0.000208   \n",
       "10       0.050704      0.001575         0.003468        0.000191   \n",
       "15       0.058885      0.005512         0.003046        0.000711   \n",
       "12       0.061927      0.002012         0.003354        0.000123   \n",
       "13       0.061335      0.002572         0.003652        0.000465   \n",
       "\n",
       "   param_max_depth param_min_samples_split  \\\n",
       "14              12                       4   \n",
       "10               9                       4   \n",
       "15              12                       5   \n",
       "12              12                       2   \n",
       "13              12                       3   \n",
       "\n",
       "                                       params  split0_test_score  \\\n",
       "14  {'max_depth': 12, 'min_samples_split': 4}           0.908213   \n",
       "10   {'max_depth': 9, 'min_samples_split': 4}           0.905797   \n",
       "15  {'max_depth': 12, 'min_samples_split': 5}           0.903382   \n",
       "12  {'max_depth': 12, 'min_samples_split': 2}           0.903382   \n",
       "13  {'max_depth': 12, 'min_samples_split': 3}           0.896135   \n",
       "\n",
       "    split1_test_score  split2_test_score  ...  split2_train_score  \\\n",
       "14           0.915459           0.946860  ...            0.977187   \n",
       "10           0.910628           0.934783  ...            0.961621   \n",
       "15           0.903382           0.932367  ...            0.975309   \n",
       "12           0.908213           0.946860  ...            0.977456   \n",
       "13           0.908213           0.942029  ...            0.977456   \n",
       "\n",
       "    split3_train_score  split4_train_score  split5_train_score  \\\n",
       "14            0.972356            0.974772            0.972088   \n",
       "10            0.957327            0.962158            0.958132   \n",
       "15            0.971014            0.973430            0.970209   \n",
       "12            0.974503            0.976382            0.973430   \n",
       "13            0.973698            0.975309            0.972893   \n",
       "\n",
       "    split6_train_score  split7_train_score  split8_train_score  \\\n",
       "14            0.975040            0.968867            0.973162   \n",
       "10            0.957059            0.954643            0.961084   \n",
       "15            0.974235            0.967525            0.971820   \n",
       "12            0.976919            0.970478            0.973698   \n",
       "13            0.976114            0.970478            0.973698   \n",
       "\n",
       "    split9_train_score  mean_train_score  std_train_score  \n",
       "14            0.971820          0.973349         0.002239  \n",
       "10            0.958937          0.959071         0.002310  \n",
       "15            0.969404          0.971900         0.002389  \n",
       "12            0.974235          0.974960         0.002197  \n",
       "13            0.973430          0.974450         0.002156  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# parameter combos with the highest validation scores\n",
    "best_accuracy = pd.DataFrame(CV_trees.cv_results_).sort_values('mean_test_score', ascending=False)\n",
    "best_accuracy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a28f5220",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_min_samples_split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.924879</td>\n",
       "      <td>0.973349</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.922705</td>\n",
       "      <td>0.959071</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.921014</td>\n",
       "      <td>0.971900</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.921014</td>\n",
       "      <td>0.974960</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.921014</td>\n",
       "      <td>0.974450</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_test_score  mean_train_score param_max_depth param_min_samples_split\n",
       "14         0.924879          0.973349              12                       4\n",
       "10         0.922705          0.959071               9                       4\n",
       "15         0.921014          0.971900              12                       5\n",
       "12         0.921014          0.974960              12                       2\n",
       "13         0.921014          0.974450              12                       3"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_accuracy[['mean_test_score', 'mean_train_score','param_max_depth','param_min_samples_split']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75793efe",
   "metadata": {},
   "source": [
    "The test scores are very similar to the LogisticRegression model results previously.  There is definitely some overfitting, since the training scores have about 30-50% the error rate of the validation folds.  This is because the best models have depths of 9 or even 12, which is deep enough to learn combinations of feature splits that don't generalize well.  On the other hand, the model is counteracting this tendency by using a minimum of 4 samples at a node in order to split on it, in the best models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb79f92",
   "metadata": {},
   "source": [
    "Let's train a model with the `max_depth=9` and `min_samples_split=4`, since it overfit less.  The results should be similar to what we just saw, but maybe the false positive rate is less than with logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7a93a57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = DecisionTreeClassifier(max_depth=9, min_samples_split=4, random_state=1234).fit(X_train, y_train)\n",
    "tree_probs = tree.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d16d08e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold set to 0.5\n",
      "Test accuracy is 0.9349\n",
      "False positive rate is 0.047\n"
     ]
    }
   ],
   "source": [
    "truths = y_test\n",
    "threshold(0.5, tree_probs, truths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55cd7419",
   "metadata": {},
   "source": [
    "It seems like the tree got a little lucky on the 10% of emails we happened to hold out. Most importantly, it seems to have a very similar FP rate as the LogReg did.  We'll just try a couple of thresholds before moving on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9783c92f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold set to 0.9\n",
      "Test accuracy is 0.9306\n",
      "False positive rate is 0.029\n"
     ]
    }
   ],
   "source": [
    "threshold(0.9, tree_probs, truths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63655f76",
   "metadata": {},
   "source": [
    "**That's essentially as powerful as the LogReg classifier -- This tree with a high threshold has higher accuracy and slightly worse false positive rate than the LogReg with high threshold.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d581ca",
   "metadata": {},
   "source": [
    "For the tree, we used cross-validation to find the best parameters, and each combination of hyperparameters was used for 10 different splits, so we can feel somewhat confident that the results will generalize to new data (as long as it's all sampled from George's emails around this point in time).  But we only got accuracy scores, not precision, so let's run a few models to test the higher threshold and see how it affects false positives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b72492a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "accs = []\n",
    "fprs = []\n",
    "thr = 0.9\n",
    "for seed in range(10):\n",
    "    X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=.1, random_state=seed)\n",
    "    tree = DecisionTreeClassifier(max_depth=9, min_samples_split=4, random_state=seed).fit(X_tr, y_tr)\n",
    "    tree_probs = tree.predict_proba(X_te)\n",
    "    preds = [p[1] > thr for p in tree_probs]\n",
    "    accs.append(accuracy_score(y_te, preds))\n",
    "    fprs.append(sum(preds - y_te == 1) / sum(y_te==0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5615d95e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold was set to 0.9\n",
      "The mean accuracy over 10 different models was 0.9035\n",
      "The mean FPR over 10 different models was 0.0415\n"
     ]
    }
   ],
   "source": [
    "print('Threshold was set to 0.9')\n",
    "print(f\"The mean accuracy over 10 different models was {round(np.mean(accs),4)}\")\n",
    "print(f\"The mean FPR over 10 different models was {round(np.mean(fprs),4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483b3c04",
   "metadata": {},
   "source": [
    "**This indicates that the single model trained on just one split got a bit lucky, with 93% accuracy and 2.9% FPR.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e6c1fe",
   "metadata": {},
   "source": [
    "The UCI website that hosts this data says:\n",
    "` False positives (marking good mail as spam) are very undesirable.  If we insist on zero false positives in the training/testing set, 20-25% of the spam passed through the filter.`\n",
    "It's unclear what \"zero false positives in the training/testing set\" means here, but we'll try to build a classifier that consistently produces no FP's.  There are 40 spams per 100 emails.  We are searching for 0% FPR on the other 60, and aiming to let through less than 20% of those 40 spams, or less than 8% of all emails, meaning we need accuracy greater than 92%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21226ff",
   "metadata": {},
   "source": [
    "The tree variants that usually perform best on these tasks are `RandomForestClassifier` and `GradientBoostingClassifier`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fb658acf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10,\n",
       "             estimator=RandomForestClassifier(n_estimators=500,\n",
       "                                              random_state=1234),\n",
       "             n_jobs=-1,\n",
       "             param_grid=[{'max_depth': [6, 9, 12],\n",
       "                          'min_samples_split': [2, 3, 4]}],\n",
       "             return_train_score=True, scoring='accuracy')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# As with all these GridSearchCV routines, training dozens of models can take awhile (~ 1 min here)\n",
    "grid_params_for_forest = [{\n",
    "    'max_depth': [6,9,12],\n",
    "    'min_samples_split': [2,3,4]}]\n",
    "\n",
    "CV_forest = GridSearchCV(estimator = RandomForestClassifier(n_estimators=500, random_state=1234), \n",
    "                         scoring='accuracy', param_grid = grid_params_for_forest,\n",
    "                               cv = 10, return_train_score=True, n_jobs=-1)\n",
    "# As with trees, we can use the unscaled data here\n",
    "CV_forest.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d2e56628",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_min_samples_split</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>...</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>split3_train_score</th>\n",
       "      <th>split4_train_score</th>\n",
       "      <th>split5_train_score</th>\n",
       "      <th>split6_train_score</th>\n",
       "      <th>split7_train_score</th>\n",
       "      <th>split8_train_score</th>\n",
       "      <th>split9_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3.536149</td>\n",
       "      <td>0.037046</td>\n",
       "      <td>0.114863</td>\n",
       "      <td>0.005025</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>{'max_depth': 12, 'min_samples_split': 2}</td>\n",
       "      <td>0.939614</td>\n",
       "      <td>0.934783</td>\n",
       "      <td>0.946860</td>\n",
       "      <td>...</td>\n",
       "      <td>0.979066</td>\n",
       "      <td>0.980140</td>\n",
       "      <td>0.979066</td>\n",
       "      <td>0.978798</td>\n",
       "      <td>0.979334</td>\n",
       "      <td>0.977724</td>\n",
       "      <td>0.978529</td>\n",
       "      <td>0.978798</td>\n",
       "      <td>0.979227</td>\n",
       "      <td>0.000833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3.575598</td>\n",
       "      <td>0.009654</td>\n",
       "      <td>0.110706</td>\n",
       "      <td>0.002506</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>{'max_depth': 12, 'min_samples_split': 3}</td>\n",
       "      <td>0.934783</td>\n",
       "      <td>0.937198</td>\n",
       "      <td>0.946860</td>\n",
       "      <td>...</td>\n",
       "      <td>0.977187</td>\n",
       "      <td>0.977456</td>\n",
       "      <td>0.976382</td>\n",
       "      <td>0.976651</td>\n",
       "      <td>0.977187</td>\n",
       "      <td>0.976651</td>\n",
       "      <td>0.977992</td>\n",
       "      <td>0.977456</td>\n",
       "      <td>0.977268</td>\n",
       "      <td>0.000564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3.292135</td>\n",
       "      <td>0.557159</td>\n",
       "      <td>0.092053</td>\n",
       "      <td>0.022855</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>{'max_depth': 12, 'min_samples_split': 4}</td>\n",
       "      <td>0.934783</td>\n",
       "      <td>0.937198</td>\n",
       "      <td>0.946860</td>\n",
       "      <td>...</td>\n",
       "      <td>0.976919</td>\n",
       "      <td>0.976651</td>\n",
       "      <td>0.975040</td>\n",
       "      <td>0.973430</td>\n",
       "      <td>0.976919</td>\n",
       "      <td>0.974772</td>\n",
       "      <td>0.974772</td>\n",
       "      <td>0.975845</td>\n",
       "      <td>0.975926</td>\n",
       "      <td>0.001321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.118102</td>\n",
       "      <td>0.048349</td>\n",
       "      <td>0.102361</td>\n",
       "      <td>0.001990</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>{'max_depth': 9, 'min_samples_split': 2}</td>\n",
       "      <td>0.934783</td>\n",
       "      <td>0.929952</td>\n",
       "      <td>0.937198</td>\n",
       "      <td>...</td>\n",
       "      <td>0.961621</td>\n",
       "      <td>0.961889</td>\n",
       "      <td>0.961084</td>\n",
       "      <td>0.961889</td>\n",
       "      <td>0.962426</td>\n",
       "      <td>0.960816</td>\n",
       "      <td>0.962158</td>\n",
       "      <td>0.961889</td>\n",
       "      <td>0.961970</td>\n",
       "      <td>0.000679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.192847</td>\n",
       "      <td>0.018279</td>\n",
       "      <td>0.103052</td>\n",
       "      <td>0.003374</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>{'max_depth': 9, 'min_samples_split': 3}</td>\n",
       "      <td>0.937198</td>\n",
       "      <td>0.929952</td>\n",
       "      <td>0.937198</td>\n",
       "      <td>...</td>\n",
       "      <td>0.961084</td>\n",
       "      <td>0.960548</td>\n",
       "      <td>0.960816</td>\n",
       "      <td>0.960548</td>\n",
       "      <td>0.960816</td>\n",
       "      <td>0.961084</td>\n",
       "      <td>0.962158</td>\n",
       "      <td>0.961621</td>\n",
       "      <td>0.961218</td>\n",
       "      <td>0.000615</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "6       3.536149      0.037046         0.114863        0.005025   \n",
       "7       3.575598      0.009654         0.110706        0.002506   \n",
       "8       3.292135      0.557159         0.092053        0.022855   \n",
       "3       3.118102      0.048349         0.102361        0.001990   \n",
       "4       3.192847      0.018279         0.103052        0.003374   \n",
       "\n",
       "  param_max_depth param_min_samples_split  \\\n",
       "6              12                       2   \n",
       "7              12                       3   \n",
       "8              12                       4   \n",
       "3               9                       2   \n",
       "4               9                       3   \n",
       "\n",
       "                                      params  split0_test_score  \\\n",
       "6  {'max_depth': 12, 'min_samples_split': 2}           0.939614   \n",
       "7  {'max_depth': 12, 'min_samples_split': 3}           0.934783   \n",
       "8  {'max_depth': 12, 'min_samples_split': 4}           0.934783   \n",
       "3   {'max_depth': 9, 'min_samples_split': 2}           0.934783   \n",
       "4   {'max_depth': 9, 'min_samples_split': 3}           0.937198   \n",
       "\n",
       "   split1_test_score  split2_test_score  ...  split2_train_score  \\\n",
       "6           0.934783           0.946860  ...            0.979066   \n",
       "7           0.937198           0.946860  ...            0.977187   \n",
       "8           0.937198           0.946860  ...            0.976919   \n",
       "3           0.929952           0.937198  ...            0.961621   \n",
       "4           0.929952           0.937198  ...            0.961084   \n",
       "\n",
       "   split3_train_score  split4_train_score  split5_train_score  \\\n",
       "6            0.980140            0.979066            0.978798   \n",
       "7            0.977456            0.976382            0.976651   \n",
       "8            0.976651            0.975040            0.973430   \n",
       "3            0.961889            0.961084            0.961889   \n",
       "4            0.960548            0.960816            0.960548   \n",
       "\n",
       "   split6_train_score  split7_train_score  split8_train_score  \\\n",
       "6            0.979334            0.977724            0.978529   \n",
       "7            0.977187            0.976651            0.977992   \n",
       "8            0.976919            0.974772            0.974772   \n",
       "3            0.962426            0.960816            0.962158   \n",
       "4            0.960816            0.961084            0.962158   \n",
       "\n",
       "   split9_train_score  mean_train_score  std_train_score  \n",
       "6            0.978798          0.979227         0.000833  \n",
       "7            0.977456          0.977268         0.000564  \n",
       "8            0.975845          0.975926         0.001321  \n",
       "3            0.961889          0.961970         0.000679  \n",
       "4            0.961621          0.961218         0.000615  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# parameter combos with the highest validation scores\n",
    "best_accuracy = pd.DataFrame(CV_forest.cv_results_).sort_values('mean_test_score', ascending=False)\n",
    "best_accuracy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c476c0de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_min_samples_split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.946135</td>\n",
       "      <td>0.979227</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.945169</td>\n",
       "      <td>0.977268</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.944203</td>\n",
       "      <td>0.975926</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.939855</td>\n",
       "      <td>0.961970</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.939614</td>\n",
       "      <td>0.961218</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_test_score  mean_train_score param_max_depth param_min_samples_split\n",
       "6         0.946135          0.979227              12                       2\n",
       "7         0.945169          0.977268              12                       3\n",
       "8         0.944203          0.975926              12                       4\n",
       "3         0.939855          0.961970               9                       2\n",
       "4         0.939614          0.961218               9                       3"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_accuracy[['mean_test_score', 'mean_train_score','param_max_depth','param_min_samples_split']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e264f60",
   "metadata": {},
   "source": [
    "Those are even better scores, but the CV search chose the most overfittable params -- highest depth and smallest split.  Since random forests are somewhat resistant to overfitting, we might try even more extreme numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6972a278",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10,\n",
       "             estimator=RandomForestClassifier(n_estimators=500,\n",
       "                                              random_state=1234),\n",
       "             n_jobs=-1,\n",
       "             param_grid=[{'max_depth': [12, 14, 16],\n",
       "                          'min_samples_split': [2, 3]}],\n",
       "             return_train_score=True, scoring='accuracy')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_params_for_forest = [{\n",
    "    'max_depth': [12,14,16],\n",
    "    'min_samples_split': [2,3]}]\n",
    "\n",
    "CV_forest = GridSearchCV(estimator = RandomForestClassifier(n_estimators=500, random_state=1234), scoring='accuracy',\n",
    "                               param_grid = grid_params_for_forest,\n",
    "                               cv = 10, return_train_score=True, n_jobs=-1)\n",
    "\n",
    "CV_forest.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b1e72a4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_min_samples_split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.950725</td>\n",
       "      <td>0.989962</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.950725</td>\n",
       "      <td>0.990821</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.948551</td>\n",
       "      <td>0.987493</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.948068</td>\n",
       "      <td>0.986098</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.946135</td>\n",
       "      <td>0.979227</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_test_score  mean_train_score param_max_depth param_min_samples_split\n",
       "5         0.950725          0.989962              16                       3\n",
       "4         0.950725          0.990821              16                       2\n",
       "2         0.948551          0.987493              14                       2\n",
       "3         0.948068          0.986098              14                       3\n",
       "0         0.946135          0.979227              12                       2"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# parameter combos with the highest validation scores\n",
    "best_accuracy = pd.DataFrame(CV_forest.cv_results_).sort_values('mean_test_score', ascending=False)\n",
    "best_accuracy[['mean_test_score', 'mean_train_score','param_max_depth','param_min_samples_split']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f0ee3475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold set to 0.5\n",
      "Test accuracy is 0.9566\n",
      "False positive rate is 0.025\n"
     ]
    }
   ],
   "source": [
    "# Use the best params from above\n",
    "forest = RandomForestClassifier(max_depth=16, min_samples_split=3,\n",
    "                                n_estimators=500, random_state=1234).fit(X_train, y_train)\n",
    "forest_probs = forest.predict_proba(X_test)\n",
    "threshold(0.5, forest_probs, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e02190",
   "metadata": {},
   "source": [
    "Raise the threshold to where no good mail gets filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "eab2546e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold set to 0.9\n",
      "Test accuracy is 0.8568\n",
      "False positive rate is 0.0\n"
     ]
    }
   ],
   "source": [
    "threshold(0.9, forest_probs, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d74e55",
   "metadata": {},
   "source": [
    "**This random forest model has an accuracy over 95% if you're OK with a 2.5% FPR, but drops to 85.7% if you need 0% FPR**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0c8a8f",
   "metadata": {},
   "source": [
    "Let's get ten more opinions before moving on to gradient boosting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1be18cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "accs = []\n",
    "fprs = []\n",
    "for seed in range(10):\n",
    "    X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=.1, random_state=seed)\n",
    "    forest = RandomForestClassifier(max_depth=16, min_samples_split=3,\n",
    "                                n_estimators=500, random_state=seed).fit(X_tr, y_tr)\n",
    "    forest_probs = forest.predict_proba(X_te)\n",
    "    accs.append(accuracy_score(y_te, [p[1] > 0.9 for p in forest_probs]))\n",
    "    fprs.append(sum([p[1] > 0.9 for p in forest_probs] - y_te == 1) / sum(y_te==0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8fea9173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold was set to 0.9\n",
      "The mean accuracy over 10 different models was 0.8486\n",
      "The mean FPR over 10 different models was 0.0022\n"
     ]
    }
   ],
   "source": [
    "print('Threshold was set to 0.9')\n",
    "print(f\"The mean accuracy over 10 different models was {round(np.mean(accs),4)}\")\n",
    "print(f\"The mean FPR over 10 different models was {round(np.mean(fprs),4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997cb1f8",
   "metadata": {},
   "source": [
    "**That won't cut it. Also it sent a couple of good emails to spam.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0beb3c22",
   "metadata": {},
   "source": [
    "Let's try gradient boosted trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7d99c070",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10,\n",
       "             estimator=GradientBoostingClassifier(n_estimators=500,\n",
       "                                                  n_iter_no_change=4,\n",
       "                                                  random_state=1234),\n",
       "             n_jobs=-1,\n",
       "             param_grid=[{'learning_rate': [0.15, 0.3], 'max_depth': [3, 4],\n",
       "                          'min_samples_split': [4, 5, 6]}],\n",
       "             return_train_score=True, scoring='accuracy')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training time and overfitting are chiefly managed by n_iter_no_change parameter\n",
    "grid_params_for_boosting = [{\n",
    "    'max_depth': [3,4],\n",
    "    'min_samples_split': [4,5,6],\n",
    "    'learning_rate':[0.15, 0.3]}]\n",
    "\n",
    "CV_boost = GridSearchCV(estimator = GradientBoostingClassifier(n_estimators=500, random_state=1234, n_iter_no_change=4),\n",
    "                        scoring='accuracy', param_grid = grid_params_for_boosting,\n",
    "                        cv = 10, return_train_score=True, n_jobs=-1)\n",
    "# use whole dataset this time, since we'll rebuild a model anyways\n",
    "CV_boost.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f2bc6afb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_min_samples_split</th>\n",
       "      <th>param_learning_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.941531</td>\n",
       "      <td>0.966384</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.940662</td>\n",
       "      <td>0.973339</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.940661</td>\n",
       "      <td>0.967688</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.940009</td>\n",
       "      <td>0.968123</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.939793</td>\n",
       "      <td>0.973581</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_test_score  mean_train_score param_max_depth param_min_samples_split  \\\n",
       "1         0.941531          0.966384               3                       5   \n",
       "5         0.940662          0.973339               4                       6   \n",
       "0         0.940661          0.967688               3                       4   \n",
       "8         0.940009          0.968123               3                       6   \n",
       "4         0.939793          0.973581               4                       5   \n",
       "\n",
       "  param_learning_rate  \n",
       "1                0.15  \n",
       "5                0.15  \n",
       "0                0.15  \n",
       "8                 0.3  \n",
       "4                0.15  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# parameter combos with the highest validation scores\n",
    "best_accuracy = pd.DataFrame(CV_boost.cv_results_).sort_values('mean_test_score', ascending=False)\n",
    "best_accuracy[['mean_test_score', 'mean_train_score','param_max_depth','param_min_samples_split','param_learning_rate']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717c2d34",
   "metadata": {},
   "source": [
    "Let's see what the FP rate is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a0f3b5a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold set to 0.5\n",
      "Test accuracy is 0.9588\n",
      "False positive rate is 0.036\n"
     ]
    }
   ],
   "source": [
    "# Use the shallowest depth and the higher min_samples_split\n",
    "booster = GradientBoostingClassifier(max_depth=3, min_samples_split=5, learning_rate=0.15,\n",
    "                                n_estimators=500, random_state=1234).fit(X_train, y_train)\n",
    "booster_probs = booster.predict_proba(X_test)\n",
    "threshold(0.5, booster_probs, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c6b3db",
   "metadata": {},
   "source": [
    "And with a higher classification threshold?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8d688090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold set to 0.9\n",
      "Test accuracy is 0.9414\n",
      "False positive rate is 0.014\n"
     ]
    }
   ],
   "source": [
    "threshold(0.9, booster_probs, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c44a03",
   "metadata": {},
   "source": [
    "**That's the most powerful classifier yet, but is still not getting 0% FPR.**  \n",
    "\n",
    "Let's see once more how it generalizes to 10 random fits and splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e366d31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "accs = []\n",
    "fprs = []\n",
    "thr = 0.9\n",
    "for rs in range(10):\n",
    "    X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=.1, random_state=rs)\n",
    "    booster = GradientBoostingClassifier(max_depth=3, min_samples_split=5, learning_rate=0.15,\n",
    "                                n_estimators=500, random_state=rs).fit(X_tr, y_tr)\n",
    "    booster_probs = booster.predict_proba(X_te)\n",
    "    preds = [p[1] > thr for p in booster_probs]\n",
    "    accs.append(accuracy_score(y_te, preds))\n",
    "    fprs.append(sum(preds - y_te == 1) / sum(y_te==0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "cc73e56d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold was set to 0.9\n",
      "The mean accuracy over 10 different models was 0.9388\n",
      "The mean FPR over 10 different models was 0.0112\n"
     ]
    }
   ],
   "source": [
    "print(f'Threshold was set to {thr}')\n",
    "print(f\"The mean accuracy over 10 different models was {round(np.mean(accs),4)}\")\n",
    "print(f\"The mean FPR over 10 different models was {round(np.mean(fprs),4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41779e5e",
   "metadata": {},
   "source": [
    "It's hard to know how to interpret these results.  We've used some moderately sophisticated algorithms and techniques, and yet we can't eliminate false positives over many repeated runs.  It makes you (us) wonder what the hosts of this dataset mean by \"insisting on zero false positives\". How is that even possible?  Did they run one single split and fit and get such a result?  Did they then try it on every possible split and fit? If we gave them George's next 1000 emails, how much would they bet on zero false positives?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca63b935",
   "metadata": {},
   "source": [
    "**Were the same features important for the GBC as for LogReg?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7ba70386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBC most important\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0.2325409507096751, 'char_freq_!:'),\n",
       " (0.19307013396261702, 'char_freq_$:'),\n",
       " (0.1174214199362354, 'word_freq_remove:'),\n",
       " (0.08773718427150339, 'word_freq_hp:'),\n",
       " (0.05811219648456616, 'word_freq_free:'),\n",
       " (0.057012463917724715, 'capital_run_length_average:'),\n",
       " (0.03435625166201122, 'word_freq_your:'),\n",
       " (0.03177906060287507, 'capital_run_length_longest:'),\n",
       " (0.028981346156235873, 'word_freq_george:'),\n",
       " (0.026140152403960816, 'word_freq_edu:')]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('GBC most important')\n",
    "sorted(list(zip(booster.feature_importances_, X.columns)), reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4320ec5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogReg most important\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(-4.342655119799001, 'word_freq_george:'),\n",
       " (-2.6306171946409926, 'word_freq_hp:'),\n",
       " (-1.9651440530264461, 'word_freq_cs:'),\n",
       " (-1.483445991016186, 'word_freq_meeting:'),\n",
       " (1.3629780202839097, 'char_freq_$:'),\n",
       " (1.1582746904568435, 'capital_run_length_longest:'),\n",
       " (-1.1264679265650326, 'word_freq_lab:'),\n",
       " (-1.1225967091537659, 'word_freq_edu:'),\n",
       " (1.0141596061160116, 'word_freq_3d:'),\n",
       " (-0.9822463528948014, 'word_freq_hpl:')]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('LogReg most important')\n",
    "sorted(list(zip(linear_reg.coef_[0], X.columns)), key=lambda x: abs(x[0]), reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f63c926",
   "metadata": {},
   "source": [
    "There's a lot of overlap, but the Gradient Boosted model leaned heavily on exclamation points, presumably to indicate spam, whereas the Linear model banked more on what indicated good emails, like the user-specific names."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a52cc96",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d0a00b",
   "metadata": {},
   "source": [
    "Constrained to a pre-fixed set of features and goals, we turned our focus to using some of the most common and powerful ML tools.  \n",
    "- Training Classifiers to generalize to unseen inputs\n",
    " - Scaling features appropriately\n",
    " - Splitting data into subsets where necessary\n",
    " - Using pipelines with cross-validation\n",
    " - Using grid search with cross-validation to optimize hyperparameters  \n",
    " \n",
    " \n",
    "- Evaluating the trained Classifiers\n",
    " - Choosing appropriate metrics \n",
    " - Interpreting repeatability/generalizability of test results\n",
    " - Avoiding test data leakage\n",
    " - Raising classification thresholds to eliminate false positives\n",
    " - Interpreting learned features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a626c235",
   "metadata": {},
   "source": [
    "Our most powerful model was the gradient boosted classifier with a high, pre-set threshold, which was 93.9% accurate on multiple data splits.  Unfortunately it still sent 1.1% of good emails to the spam folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179d91d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
